# 决策树 

## **决策树是如何工作的**

决策树(Decision Tree)是一种**非参数的有监督学习方法**，它能够从一系列有特征和标签的数据中总结出决策规 则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。

决策树算法容易理解，适用各种数据，在解决各 种问题时都有良好表现，尤其是以树模型为核心的各种集成算法，在各个行业和领域都有广泛的应用。

决策树算法的本质是一种图结构，

**关键概念:节点**

> **根节点:**没有进边，有出边。包含最初的，针对特征的提问。 
>
> **中间节点:**既有进边也有出边，进边只有一条，出边可以有很多条。都是针对特征的提问。 
>
> **叶子节点:**有进边，没有出边，每个叶子节点都是一个类别标签。 
>
> **子节点和父节点:**在两个相连的节点中，更接近根节点的是父节点，另一个是子节点。
>
> 

**决策树算法的核心**是要解决两个问题: 

1)如何从数据表中找出最佳节点和最佳分枝? 

2)如何让决策树停止生长，防止过拟合?

几乎所有决策树有关的模型调整方法，都围绕这两个问题展开。这两个问题背后的原理十分复杂，我们会在讲解模型参数和属性的时候为大家简单解释涉及到的部分。在这门课中，我会尽量避免让大家太过深入到决策树复杂的原理和数学公式中(尽管决策树的原理相比其他高级的算法来说是非常简单了)，这门课会专注于实践和应用。如果大家希望理解更深入的细节，建议大家在听这门课之前还是先去阅读和学习一下决策树的原理。

## sklearn中的决策树 模块sklearn.tree

sklearn中决策树的类都在”tree“这个模块之下。这个模块总共包含五个类:

|   类   |   功能   |
| ---- | ---- |
|tree.DecisionTreeClassifie | 分类树 |
|tree.DecisionTreeRegressor | 回归树 |
|tree.export_graphviz  | 将生成的决策树导出为DOT格式，画图专用 高随机版本的分类树 |
|tree.ExtraTreeRegressor | 高随机版本的回归树 |
|tree.ExtraTreeClassifier | 高随机版本的分类树 |

## 分类树

主要调用流程

```python 
from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
result = clf.score(X_test,y_test)
```

[DecisionTreeClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)有非常多的参数，这里说几个重要的。

**重要参数**

1. criterion 这个参数用来决定不纯度的计算方法，主要有entropy（信息墒）gini（基尼系数）

2. random_state & splitter ,都是用来控制决策树中的随机选项的，random_state相当于固定随机种子，而splitter有两种输入值，输入”best"，决策树在分枝时虽然随机，但是还是会 优先选择更重要的特征进行分枝(重要性可以通过属性feature_importances_查看)，输入“random"，决策树在 分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这 也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能 性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合。

3. > 剪枝参数：重中之重

   - max_depth,限制树的最大深度，超过设定深度的树枝全部剪掉

     这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所 以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效 果再决定是否增加设定深度。

   - min_samples_leaf & min_samples_split 

     min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分

     枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生

     一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。

     这个参数的数量设置得太小会引 起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很 大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题 中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。

     min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则 分枝就不会发生。

   - max_features & min_impurity_decrease

     一般max_depth使用，用作树的”精修“

     max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工， max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量 而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型 学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。

     min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的 功能，在0.19版本之前时使用min_impurity_split。

     

   - class_weight & min_weight_fraction_leaf

     完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要 判断“一个办了信用卡的人是否会违约”，就是是vs否(1%:99%)的比例。这种分类状况下，即便模型什么也不 做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给 少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给 与数据集中的所有标签相同的权重。

     有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_ weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数(例如min_weight_ fraction_leaf)将比不知道样本权重的标准(比如min_samples_leaf)更少偏向主导类。如果样本是加权的，则使 用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。

## 重要属性和接口 

属性是在模型训练之后，能够调用查看的模型的各种性质。对决策树来说，最重要的是feature_importances_，能 够查看各个特征对模型的重要性。

sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，几乎对每个算法都可以使用。除了 这两个接口之外，决策树最常用的接口还有apply和predict。apply中输入测试集返回每个测试样本所在的叶子节 点的索引，predict输入测试集返回每个测试样本的标签。返回的内容一目了然并且非常容易。

> apply返回每个测试样本所在的叶子节点的索引 clf.apply(Xtest)
> predict返回每个测试样本的分类/回归结果 clf.predict(Xtest)

## 回归树

回归树的DecisionTreeRegressor()参数意义跟分类树是一样的，只需要注意criterion可选为三种标准

- mse
- Friedman_mse
- mae

分类树是根据不纯度来分枝的，而回归树是根据误差。

> 回归树的接口score返回的是R2，并不是MSE。
>
> sklearn在计算MSE的时候，都用得是负数。

## 决策树算法: ID3, C4.5, C5.0 和 CART

所有种类的决策树算法有哪些以及它们之间的区别？scikit-learn 中实现何种算法呢？

[ID3](https://en.wikipedia.org/wiki/ID3_algorithm)（Iterative Dichotomiser 3）由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。

C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。

C5.0 是 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。

[CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)（Classification and Regression Trees （分类和回归树））与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。

scikit-learn 使用 **CART 算法的优化版本**。

> 最后总结：调包无罪，真正能够会调包也是不容易的，每个参数有什么意义，能够怎么用，这个函数背后用的什么算法逻辑，一定要看官方文档！

https://scikit-learn.org/stable/

