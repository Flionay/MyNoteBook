# è®­ç»ƒæ¨¡å‹ä»£ç æµç¨‹

Pytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼šè„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œå‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œç±»å½¢å¼è®­ç»ƒå¾ªç¯ã€‚



## 1. è„šæœ¬å¾ªç¯é£æ ¼

```python
net = nn.Sequential()
net.add_module("conv1",nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3))
net.add_module("pool1",nn.MaxPool2d(kernel_size = 2,stride = 2))
net.add_module("conv2",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5))
net.add_module("pool2",nn.MaxPool2d(kernel_size = 2,stride = 2))
net.add_module("dropout",nn.Dropout2d(p = 0.1))
net.add_module("adaptive_pool",nn.AdaptiveMaxPool2d((1,1)))
net.add_module("flatten",nn.Flatten())
net.add_module("linear1",nn.Linear(64,32))
net.add_module("relu",nn.ReLU())
net.add_module("linear2",nn.Linear(32,10))

print(net)
```

```python

import datetime
import numpy as np 
import pandas as pd 
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data
    return accuracy_score(y_true,y_pred_cls)

loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=net.parameters(),lr = 0.01)
metric_func = accuracy
metric_name = "accuracy"

epochs = 3
log_step_freq = 100

dfhistory = pd.DataFrame(columns = ["epoch","loss",metric_name,"val_loss","val_"+metric_name]) 
print("Start Training...")
nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
print("=========="*8 + "%s"%nowtime)

for epoch in range(1,epochs+1):  

    # 1ï¼Œè®­ç»ƒå¾ªç¯-------------------------------------------------
    net.train()
    loss_sum = 0.0
    metric_sum = 0.0
    step = 1

    for step, (features,labels) in enumerate(dl_train, 1):

        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()

        # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
        predictions = net(features)
        loss = loss_func(predictions,labels)
        metric = metric_func(predictions,labels)

        # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
        loss.backward()
        optimizer.step()

        # æ‰“å°batchçº§åˆ«æ—¥å¿—
        loss_sum += loss.item()
        metric_sum += metric.item()
        if step%log_step_freq == 0:   
            print(("[step = %d] loss: %.3f, "+metric_name+": %.3f") %
                  (step, loss_sum/step, metric_sum/step))

    # 2ï¼ŒéªŒè¯å¾ªç¯-------------------------------------------------
    net.eval()
    val_loss_sum = 0.0
    val_metric_sum = 0.0
    val_step = 1

    for val_step, (features,labels) in enumerate(dl_valid, 1):
        with torch.no_grad():
            predictions = net(features)
            val_loss = loss_func(predictions,labels)
            val_metric = metric_func(predictions,labels)

        val_loss_sum += val_loss.item()
        val_metric_sum += val_metric.item()

    # 3ï¼Œè®°å½•æ—¥å¿—-------------------------------------------------
    info = (epoch, loss_sum/step, metric_sum/step, 
            val_loss_sum/val_step, val_metric_sum/val_step)
    dfhistory.loc[epoch-1] = info

    # æ‰“å°epochçº§åˆ«æ—¥å¿—
    print(("\nEPOCH = %d, loss = %.3f,"+ metric_name + \
          "  = %.3f, val_loss = %.3f, "+"val_"+ metric_name+" = %.3f") 
          %info)
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

print('Finished Training...')

```

## 2. å‡½æ•°æ¨¡å¼

```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x
net = Net()
```

```python
def train_step(model,features,labels):

    # è®­ç»ƒæ¨¡å¼ï¼Œdropoutå±‚å‘ç”Ÿä½œç”¨
    model.train()

    # æ¢¯åº¦æ¸…é›¶
    model.optimizer.zero_grad()

    # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
    predictions = model(features)
    loss = model.loss_func(predictions,labels)
    metric = model.metric_func(predictions,labels)

    # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
    loss.backward()
    model.optimizer.step()

    return loss.item(),metric.item()

@torch.no_grad()
def valid_step(model,features,labels):

    # é¢„æµ‹æ¨¡å¼ï¼Œdropoutå±‚ä¸å‘ç”Ÿä½œç”¨
    model.eval()

    predictions = model(features)
    loss = model.loss_func(predictions,labels)
    metric = model.metric_func(predictions,labels)

    return loss.item(), metric.item()

```

```python
def train_model(model,epochs,dl_train,dl_valid,log_step_freq):

    metric_name = model.metric_name
    dfhistory = pd.DataFrame(columns = ["epoch","loss",metric_name,"val_loss","val_"+metric_name]) 
    print("Start Training...")
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("=========="*8 + "%s"%nowtime)

    for epoch in range(1,epochs+1):  

        # 1ï¼Œè®­ç»ƒå¾ªç¯-------------------------------------------------
        loss_sum = 0.0
        metric_sum = 0.0
        step = 1

        for step, (features,labels) in enumerate(dl_train, 1):

            loss,metric = train_step(model,features,labels)

            # æ‰“å°batchçº§åˆ«æ—¥å¿—
            loss_sum += loss
            metric_sum += metric
            if step%log_step_freq == 0:   
                print(("[step = %d] loss: %.3f, "+metric_name+": %.3f") %
                      (step, loss_sum/step, metric_sum/step))

        # 2ï¼ŒéªŒè¯å¾ªç¯-------------------------------------------------
        val_loss_sum = 0.0
        val_metric_sum = 0.0
        val_step = 1

        for val_step, (features,labels) in enumerate(dl_valid, 1):

            val_loss,val_metric = valid_step(model,features,labels)

            val_loss_sum += val_loss
            val_metric_sum += val_metric

        # 3ï¼Œè®°å½•æ—¥å¿—-------------------------------------------------
        info = (epoch, loss_sum/step, metric_sum/step, 
                val_loss_sum/val_step, val_metric_sum/val_step)
        dfhistory.loc[epoch-1] = info

        # æ‰“å°epochçº§åˆ«æ—¥å¿—
        print(("\nEPOCH = %d, loss = %.3f,"+ metric_name + \
              "  = %.3f, val_loss = %.3f, "+"val_"+ metric_name+" = %.3f") 
              %info)
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print("\n"+"=========="*8 + "%s"%nowtime)

    print('Finished Training...')
    return dfhistory

```



## 3. Torchkeras

https://github.com/lyhue1991

è¿™ä½å¤§ä½¬è®²pytorchè¿›è¡Œäº†è¿›ä¸€æ­¥å°è£…ï¼Œä½¿å¾—torchè®­ç»ƒå¯ä»¥åƒkerasä¸€æ ·ç®€å•ï¼Œæºç ä¹ŸåŸºæœ¬å¯ä»¥è¯»æ‡‚ã€‚æœ¬Torchç³»åˆ—ä¹Ÿæ¥è‡ªä»–çš„20å¤©eatpytorchç³»åˆ—ï¼Œè¡¨ç¤ºæ•¬æ„ğŸ’ªï¼

orchkeras æ˜¯åœ¨pytorchä¸Šå®ç°çš„ä»¿kerasçš„é«˜å±‚æ¬¡Modelæ¥å£ã€‚æœ‰äº†å®ƒï¼Œä½ å¯ä»¥åƒKerasé‚£æ ·ï¼Œ**å¯¹pytorchæ„å»ºçš„æ¨¡å‹è¿›è¡Œsummaryï¼Œcompileï¼Œfitï¼Œevaluate , predictäº”è¿å‡»**ã€‚ä¸€åˆ‡éƒ½åƒè¡Œäº‘æµæ°´èˆ¬è‡ªç„¶ã€‚

ä¸è¿‡æˆ‘è¿™é‡Œå¿…é¡»è¦æé†’çš„æ˜¯ï¼Œç®€ä¾¿çš„åŒæ—¶ï¼Œå¾€å¾€å¸¦æ¥äº†æ›´å¤šçš„å¤æ‚ã€‚å…è´¹çš„å¾€å¾€æ˜¯æœ€è´µçš„ã€‚

ç”¨åˆ°çš„è¯å»[github](https://github.com/lyhue1991/torchkeras)æŸ¥å°±è¡Œï¼Œè¿™é‡Œå°±ä¸å†™äº†ï¼Œè¿˜æ˜¯å»ºè®®ç”¨åŸç”Ÿæ€ã€‚



